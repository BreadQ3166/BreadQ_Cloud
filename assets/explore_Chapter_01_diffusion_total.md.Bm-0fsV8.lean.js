import{_ as r,c as n,G as l,a6 as s,B as o,o as p}from"./chunks/framework.BSED1jvi.js";const k=JSON.parse('{"title":"前言","description":"","frontmatter":{},"headers":[],"relativePath":"explore/Chapter_01/diffusion/total.md","filePath":"explore/Chapter_01/diffusion/total.md"}'),c={name:"explore/Chapter_01/diffusion/total.md"};function u(f,i,h,g,d,m){const t=o("confetti"),a=o("Waline"),e=o("Linkcard");return p(),n("div",null,[l(t),l(a),l(e,{url:"/explore/Chapter_01/diffusion/total",title:"🥝Stable Diffusion的学习小记",description:"AI绘画的日常素材积累",logo:"/logo_1.jpg"}),i[0]||(i[0]=s('<hr><h1 id="前言" tabindex="-1">前言 <a class="header-anchor" href="#前言" aria-label="Permalink to &quot;前言&quot;">​</a></h1><blockquote><p>🍊Hello，各位好，我是面包！</p><p>这篇文档集合了面包的Stable diffusion的学习经历。</p></blockquote><h2 id="🍅常用网站" tabindex="-1">🍅常用网站 <a class="header-anchor" href="#🍅常用网站" aria-label="Permalink to &quot;🍅常用网站&quot;">​</a></h2><p>C站【微调模型】：<a href="https://civitai.com" target="_blank" rel="noreferrer">https://civitai.com</a></p><p>huggingface【大模型】：<a href="https://huggingface.co" target="_blank" rel="noreferrer">https://huggingface.co</a></p><p>openart【comfyui工作流】：<a href="https://openart.ai" target="_blank" rel="noreferrer">https://openart.ai</a></p><p>github【comfyui插件】:<a href="https://github.com" target="_blank" rel="noreferrer">https://github.com</a></p><h2 id="🥛sable-diffusion的主流gui" tabindex="-1">🥛Sable Diffusion的主流GUI <a class="header-anchor" href="#🥛sable-diffusion的主流gui" aria-label="Permalink to &quot;🥛Sable Diffusion的主流GUI&quot;">​</a></h2><ol><li>webui-窗口式</li></ol><p>webui是将Stable Diffusion的工作流程以简洁、可视化的方式展示在网页端的图形化用户操作界面</p><ol><li>comfyui-节点式</li></ol><p>comfyui是构建在Stable Diffusion之上的基于节点的图形化用户操作界面，类似于通过节点进行编程</p><p>别乱更新comfyui</p><p>高级节点最好链接高级节点，比如controlnet应用高级最好连接controlnet模型加载器（高级）</p><hr><h4 id="comfyui安装插件报错" tabindex="-1">ComfyUI安装插件报错 <a class="header-anchor" href="#comfyui安装插件报错" aria-label="Permalink to &quot;ComfyUI安装插件报错&quot;">​</a></h4><ol><li>没有安装git或没有把git放进环境变量中</li><li>模型下载网址huggingface被墙了</li></ol><div class="tip custom-block"><p class="custom-block-title">小纸条</p><ol><li>科学上网|登录国内镜像网站hr-mirror下载-&gt;手动下载再将模型和配置文件放进指定目录里面</li><li>comfyui的模型安装路径去插件的github界面去找-&gt;models目录下新建与插件名字相同的文件夹，然后把.config和.bin文件放进去</li></ol></div><p>comfyuibox</p><p>stableswarmUI</p><h5 id="🍾发现问题" tabindex="-1">🍾发现问题 <a class="header-anchor" href="#🍾发现问题" aria-label="Permalink to &quot;🍾发现问题&quot;">​</a></h5><ol><li>练物品那里，填写tag的那个地方，应该写一个触发词，比如说小金人，然后不要对物品本身进行描述，但是可以要描述物品的分类和构图，比如说一个奖杯、静物nohuman，然后换的背景也有点问题，好让它能够符合光影，比如第一个正光的小金人，就可以直接白色背景，而第二个应该是右亮左暗的渐变的灰色基调背景，第一排最右边应该用暗红色基调背景以此类推，就PS里拖个渐变的事，背景可以全部用“纯色背景”这个tag来进行描述。另外，需要在tag里面加入物品的视角描述，最后再加几张图是UP主手捧奖杯的，让AI知道这玩意儿是个奖杯，最好用单独的文件夹，加上单独的repeat数，然后再进行后续调整。不过我是不建议使用Lora去做物品上身的，这玩意儿，我们小团队至少研究了半年多，网上也有不少的人在研究，曾经也在B站上火过不少视频，但现在基本都销声匿迹了，问题就出在lora自带的泛化性，要么就过拟合，完全上不了身，换不了模特，要么就细节上会出现些许丢失，而电商环境的话，他们是连文理都得要求一模一样，99.99%相似他们都是不能接受的。【来源Bilibili评论区】</li></ol><p>我借鉴一下up主的想法总结了一些现在已知的训练物品<a href="https://search.bilibili.com/all?from_source=webcommentline_search&amp;keyword=lora&amp;seid=433046723051418825" target="_blank" rel="noreferrer">lora</a>思路希望有用：1.最好有三组照片，一组20张每张50训练</p><ol start="2"><li>最好各角度有光阴效果图一张。这个当一组照片用。还有一组用up主的想法抠图物品，还有最后一组是用杂物笼罩lora物品让ai自己学习。 3.lora.sh似乎需要微调。本人不懂怎么调，只好招搬前进四up主的思路。</li><li>以人物lora为例，多打标的话，出图就非常依赖提示词标签，多lora同时使用经常会因为提示词互相产生干扰，而且如果使用一个lora需要大量提示词的话，感觉并不是一个好的lora。 好处是这样训练的模型，可以像抽积木一样删改细节，每个提示词对应细节保留的更好，比如纹身，拟合的很满意。 少打标的话，在细节上就会有很多丢失，比如会出现纹身消失或者完全不拟合，这种lora因为没有给上标签，希望ai学习的地方没有学习，很多东西也固定了修改不了了，lora出炉后才能发现问题，我还没实践出解决办法。 好处是但是出图整体感觉确实更好，几个提示词就可以出图了，和其他lora冲突的情况我遇到的更少，更容易调和。</li></ol><p>我也去c站上下载用过其他人分享的模型，有些仅给了一个触发词就有非常好的效果，有些没有识别到训练标签，但是仅加上lora就可以有非常好的效果，还有就是多触发词的，这种lora单独使用效果最好。</p><h3 id="🍹模型分析" tabindex="-1">🍹模型分析 <a class="header-anchor" href="#🍹模型分析" aria-label="Permalink to &quot;🍹模型分析&quot;">​</a></h3><ol><li><h6 id="florence-2" tabindex="-1">Florence-2 <a class="header-anchor" href="#florence-2" aria-label="Permalink to &quot;Florence-2&quot;">​</a></h6></li></ol><p>  Florence-2 是微软于 2024 年 6 月发布的一个基础视觉语言模型。该模型极具吸引力，因为它尺寸很小 (0.2B 及 0.7B) 且在各种计算机视觉和视觉语言任务上表现出色。</p><p>  Florence 开箱即用支持多种类型的任务，包括: 看图说话、目标检测、OCR 等等。虽然覆盖面很广，但仍有可能你的任务或领域不在此列，也有可能你希望针对自己的任务更好地控制模型输出。此时，你就需要微调了！</p><p>  原文链接：<a href="https://blog.csdn.net/HuggingFace/article/details/140453082" target="_blank" rel="noreferrer">https://blog.csdn.net/HuggingFace/article/details/140453082</a></p><h3 id="comfyui节点报错" tabindex="-1">comfyui节点报错 <a class="header-anchor" href="#comfyui节点报错" aria-label="Permalink to &quot;comfyui节点报错&quot;">​</a></h3><ol><li>尝试更新节点</li><li>未下载模型</li><li>图片尺寸不符</li><li>模型不匹配</li></ol><h4 id="吐槽" tabindex="-1">吐槽 <a class="header-anchor" href="#吐槽" aria-label="Permalink to &quot;吐槽&quot;">​</a></h4><p>傻逼节点硬控我三个小时，高级K采样器&lt;-&gt;高级ControlNet模型加载器</p><div class="tip custom-block"><p class="custom-block-title">yolo模型</p><p>要通过Stable Diffusion、ComfyUI、LoRA等AI绘画工具来生成现实中的基本标志物在不同场景下的仿真图片，并将其用于YOLO训练，最后将模型优化并部署到嵌入式平台上，涉及以下几个步骤：</p></div><ol><li><strong>🍈生成仿真场景图片</strong></li></ol><hr><ul><li><p><strong>使用Stable Diffusion和ComfyUI</strong>：</p><ul><li><p><strong>Stable Diffusion</strong>：这是一个强大的文生图模型，可以通过文本提示生成图像。你需要提供准确的文本描述来生成包含标志物的场景图片。例如，描述一个交通标志在城市街道上的情景。稳定扩散允许你生成高分辨率的图像，这对于后续的YOLO训练非常有利。</p></li><li><p><strong>ComfyUI</strong>：这是一个基于节点的界面，可以更灵活地定制生成过程，便于调整参数以生成你需要的特定场景。可以通过ComfyUI设置不同的节点来控制图像的生成过程，比如调整模型的Checkpoint、VAE、Clip等。</p></li></ul></li><li><p><strong>使用LoRA</strong>：</p><ul><li>LoRA（Low-Rank Adaptation）用于微调模型，可以在保持基础模型稳定性的同时引入新的概念或风格。通过训练LoRA模型，你可以让模型更好地识别特定场景中的标志物。</li></ul></li></ul><ol start="2"><li><strong>🍇图像用于YOLO训练</strong></li></ol><ul><li><p><strong>数据准备</strong>：</p><ul><li>生成的图片需要标注。使用标注工具（如LabelImg）标记出图像中标志物的位置和类别，生成YOLO格式的标注文件。</li></ul></li><li><p><strong>模型训练</strong>：</p><ul><li>使用这些标注的图片训练YOLO模型。可以选择YOLO的各种版本（如YOLOv5, YOLOv8）根据你的需求选择最适合的。训练过程中，调整超参数如学习率、batch size等以优化模型性能。</li></ul></li></ul><ol start="3"><li><strong>🍠模型优化和简化</strong></li></ol><ul><li><p><strong>模型量化</strong>：</p><ul><li>量化可以将模型中的浮点数转换为低精度整数，从而减少模型大小和计算量，适合嵌入式平台。可以使用INT8量化等方法。</li></ul></li><li><p><strong>模型剪枝</strong>：</p><ul><li>通过剪枝删除模型中的不必要连接或节点，降低模型复杂度。可以基于重要性评分来决定剪枝哪些部分。</li></ul></li><li><p><strong>知识蒸馏</strong>：</p><ul><li>利用一个大型的预训练模型（教师模型）来指导一个小型模型（学生模型）的训练，使小型模型性能接近大型模型。</li></ul></li></ul><ol start="4"><li><strong>🍕部署到嵌入式平台</strong></li></ol><ul><li><strong>优化后的模型部署</strong>： <ul><li>将简化后的模型编译为适合嵌入式设备的格式（如TensorRT、TFLite等），以便在资源受限的环境中运行。这包括将模型转换为适合特定硬件的格式，确保在嵌入式设备上也能实现实时或接近实时的检测。</li></ul></li><li><strong>测试与验证</strong>： <ul><li>在嵌入式设备上进行实际测试，确保模型在硬件上能够以低延迟和高准确度检测标志物。</li></ul></li></ul><div class="warning custom-block"><p class="custom-block-title">示例</p><p>假设你需要生成“禁止停车”标志在城市街道上的图像。</p><ul><li><strong>描述</strong>：使用Stable Diffusion生成一个城市夜景的图像，背景是繁忙的街道，中央有一块“禁止停车”标志，该标志清晰可见且在灯光下反射光芒。你可以使用ComfyUI调整图像的亮度和对比度，以确保标志在各种光照条件下都可被YOLO模型识别。</li><li><strong>后续训练</strong>：这些生成的图像用于训练YOLO模型，模型通过学习这些场景来识别标志物的形状、大小和位置。</li></ul><p>  这个过程涉及从图像生成到模型优化的各个方面，需要对AI绘画工具和深度学习方法有深入的理解和实践经验。   对利用confyui生成yolov11的图片数据集，并自动标注数据，最后进行模型部署到树莓派等嵌入式平台上的全自动流程。</p></div>',46))])}const _=r(c,[["render",u]]);export{k as __pageData,_ as default};
